{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSV for testing scenarios CATE opensearch query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code automatically creates a csv file with testing scenarios. Based on the metatdata of each dataset collection a short temporal subset, a subset of up to 3 variables and a spatial subset of 1°lat x 1°lon is defined. This code should only be run when changes of the open search portal have occured, otherwise the testing scenario csv should be always reused in order to have consecutive testing results based on the same scenarios which then allow to see the changes between test runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cate.core.ds import DATA_STORE_REGISTRY\n",
    "import cate.ops as ops\n",
    "from cate.util.monitor import ConsoleMonitor\n",
    "from cate.core import ds\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from csv import DictWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dict_as_row(file_name, dict_of_elem, field_names):\n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        dict_writer = DictWriter(write_obj, fieldnames=field_names, delimiter=';')\n",
    "        # Add dictionary as wor in the csv\n",
    "        dict_writer.writerow(dict_of_elem)\n",
    "\n",
    "\n",
    "def update_csv(results_csv, header_row, results_for_dataset_collection):\n",
    "    if not os.path.isfile(results_csv):\n",
    "        with open(results_csv, 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=';')\n",
    "            writer.writerow(header_row)\n",
    "    append_dict_as_row(results_csv, results_for_dataset_collection, header_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = DATA_STORE_REGISTRY.get_data_store('esa_cci_odp_os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data source esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-21.r1 already included. Will omit this one.\n"
     ]
    }
   ],
   "source": [
    "datasets = data_store.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the output file of the test scenarios csv\n",
    "test_scenarios_csv = 'test_scenarios_after_release_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scenarios_header_row = ['dataset_collection','temporal_subset','variables_subset','spatial_subset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset collection a row is created with the specification for each collumn\n",
    "test_rows = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esacci.ICESHEETS.satellite-orbit-frequency.L4.GLL.multi-sensor.multi-platform.VARIOUS.v1-3.r1 has no file list. dataset._file_list :\n",
      "[]\n",
      "esacci.FIRE.mon.L3S.BA.MODIS.Terra.MODIS_TERRA.v5-1.pixel has no file list. dataset._file_list :\n",
      "[]\n",
      "esacci.ICESHEETS.mon.IND.GMB.GRACE-instrument.GRACE.VARIOUS.1-3.time_series has no file list. dataset._file_list :\n",
      "[]\n",
      "esacci.FIRE.mon.L3S.BA.MSI-(Sentinel-2).Sentinel-2A.MSI.v1-1.pixel has no file list. dataset._file_list :\n",
      "[]\n",
      "esacci.ICESHEETS.yr.L4.CFL.multi-sensor.multi-platform.VARIOUS.v3-0.r1 has no file list. dataset._file_list :\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    dataset.update_file_list()\n",
    "#    print(dataset.id)\n",
    "    test_rows['dataset_collection'] = dataset.id\n",
    "    try:\n",
    "        time_range = tuple(t.strftime('%Y-%m-%d') for t in [dataset._file_list[0][1], dataset._file_list[1][2]])\n",
    "    except:\n",
    "        try:\n",
    "            time_range = tuple(t.strftime('%Y-%m-%d') for t in [dataset._file_list[0][1], dataset._file_list[0][2]])\n",
    "        except:\n",
    "            print(f'{dataset.id} has no file list. dataset._file_list :')\n",
    "            print(dataset._file_list)\n",
    "            continue\n",
    "            \n",
    "    test_rows['temporal_subset'] = time_range\n",
    "    var_list = []\n",
    "    if len(dataset.meta_info['variables']) > 3:\n",
    "        while len(var_list) < 1:\n",
    "            for var in random.choices(dataset.meta_info['variables'], k=3):\n",
    "                if not ('longitude' in var['name'] or 'latitude' in var['name'] or 'bounds' in var['name'] or 'bnds' in var['name']) and var['name'] not in var_list:\n",
    "                    var_list.append(var['name'])\n",
    "    else:\n",
    "        for var in dataset.meta_info['variables']:\n",
    "            if not ('longitude' in var['name'] or 'latitude' in var['name'] or 'bounds' in var['name'] or 'bnds' in var['name']) and var['name'] not in var_list:\n",
    "                var_list.append(var['name'])\n",
    "    test_rows['variables_subset'] = var_list\n",
    "#    print(var_list)\n",
    "    indx = random.uniform(float(dataset.meta_info['bbox_minx']), float(dataset.meta_info['bbox_maxx']))\n",
    "    indy = random.uniform(float(dataset.meta_info['bbox_miny']), float(dataset.meta_info['bbox_maxy']))\n",
    "    if indx == float(dataset.meta_info['bbox_maxx']):\n",
    "        indx = indx-1\n",
    "    if indy == float(dataset.meta_info['bbox_maxy']):\n",
    "        indy = indy-1\n",
    "    if indx > 0 and indy > 0:  \n",
    "        test_rows['spatial_subset'] = f'{indx}, {indy}, {(indx+1)}, {(indy+1)}'\n",
    "    elif indx < 0 and indy < 0:\n",
    "        test_rows['spatial_subset'] = f'{(indx-1)}, {(indy-1)}, {indx}, {indy}'\n",
    "    elif indx < 0:\n",
    "        test_rows['spatial_subset'] = f'{(indx-1)}, {indy},{indx}, {(indy+1)}'\n",
    "    elif indy < 0:\n",
    "        test_rows['spatial_subset'] = f'{indx}, {(indy-1)}, {(indx+1)}, {indy}'\n",
    "\n",
    "#    print(test_rows['spatial_subset'])\n",
    "    update_csv(test_scenarios_csv, test_scenarios_header_row, test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
