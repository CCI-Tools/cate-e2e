<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite errors="0" failures="36" hostname="alicja" name="pytest" skipped="0" tests="175" time="25033.832" timestamp="2020-05-28T13:13:35.293553"><testcase cci_project="OZONE" classname="test_data_support" data_type="NP" dataset="esacci.OZONE.mon.L3.NP.multi-sensor.multi-platform.MERGED.fv0002.r1" file="test_data_support.py" files="36" line="94" name="test_open_local[remote_dataset0]" processing_level="L3" product_version="fv0002" sensor_id="multi-sensor" test_time_coverage="(&apos;2007-05-01&apos;, &apos;2007-08-01&apos;)" time="5.512" time_coverage="(&apos;1997-01-04&apos;, &apos;2008-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.GOMOS.Envisat.GOMOS_ENVISAT.v0001.r1" file="test_data_support.py" files="10" line="94" name="test_open_local[remote_dataset1]" processing_level="L3" product_version="v0001" sensor_id="GOMOS" test_time_coverage="(&apos;2005-01-01&apos;, &apos;2011-01-01&apos;)" time="3.231" time_coverage="(&apos;2002-01-01&apos;, &apos;2011-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.OSIRIS.ODIN.OSIRIS_ODIN.v0001.r1" file="test_data_support.py" files="12" line="94" name="test_open_local[remote_dataset2]" processing_level="L3" product_version="v0001" sensor_id="OSIRIS" test_time_coverage="(&apos;2012-01-01&apos;, &apos;2012-01-01&apos;)" time="0.391" time_coverage="(&apos;2001-01-01&apos;, &apos;2012-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.ADV.2-30.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset3]" processing_level="L2P" product_version="2-30" sensor_id="AATSR" test_time_coverage="(&apos;2002-09-21&apos;, &apos;2002-09-23&apos;)" time="20.959" time_coverage="(&apos;2002-07-24&apos;, &apos;2004-08-27&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.03-02.r1" file="test_data_support.py" files="2570" line="94" name="test_open_local[remote_dataset4]" processing_level="L3C" product_version="03-02" sensor_id="ATSR-2" test_time_coverage="(&apos;1995-08-05&apos;, &apos;1995-08-28&apos;)" time="23.396" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-23&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ATSR2.04-01-.r1" file="test_data_support.py" files="1082" line="94" name="test_open_local[remote_dataset5]" processing_level="L3C" product_version="04-01-" sensor_id="ATSR-2" test_time_coverage="(&apos;1997-02-19&apos;, &apos;1997-03-09&apos;)" time="18.271" time_coverage="(&apos;1995-08-02&apos;, &apos;2003-06-21&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ATSR2.04-01_seg-.r1" file="test_data_support.py" files="100" line="94" name="test_open_local[remote_dataset6]" processing_level="L3C" product_version="04-01_seg-" sensor_id="ATSR-2" test_time_coverage="(&apos;1995-07-05&apos;, &apos;2001-02-16&apos;)" time="23.352" time_coverage="(&apos;1995-06-02&apos;, &apos;2002-04-12&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.03-02.r1" file="test_data_support.py" files="89" line="94" name="test_open_local[remote_dataset7]" processing_level="L3C" product_version="03-02" sensor_id="ATSR-2" test_time_coverage="(&apos;1997-03-01&apos;, &apos;1997-05-01&apos;)" time="7.602" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ATSR2.04-01-.r1" file="test_data_support.py" files="38" line="94" name="test_open_local[remote_dataset8]" processing_level="L3C" product_version="04-01-" sensor_id="ATSR-2" test_time_coverage="(&apos;1999-05-01&apos;, &apos;2000-01-01&apos;)" time="5.476" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ATSR2.04-01_seg-.r1" file="test_data_support.py" files="4" line="94" name="test_open_local[remote_dataset9]" processing_level="L3C" product_version="04-01_seg-" sensor_id="ATSR-2" test_time_coverage="(&apos;2001-04-01&apos;, &apos;2001-06-01&apos;)" time="5.757" time_coverage="(&apos;1995-07-01&apos;, &apos;2001-06-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1" file="test_data_support.py" files="3457" line="94" name="test_open_local[remote_dataset10]" processing_level="L3C" product_version="03-02" sensor_id="AATSR" test_time_coverage="(&apos;2009-06-26&apos;, &apos;2009-07-09&apos;)" time="17.099" time_coverage="(&apos;2002-07-24&apos;, &apos;2012-04-09&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1" file="test_data_support.py" files="118" line="94" name="test_open_local[remote_dataset11]" processing_level="L3C" product_version="03-02" sensor_id="AATSR" test_time_coverage="(&apos;2007-10-01&apos;, &apos;2007-12-01&apos;)" time="6.332" time_coverage="(&apos;2002-07-01&apos;, &apos;2012-04-01&apos;)" time_frequency="month"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.MODIS.Aqua.MODIS_AQUA.2-0.r1" file="test_data_support.py" files="149" line="94" name="test_open_local[remote_dataset12]" processing_level="L3C" product_version="2-0" sensor_id="MODIS" test_time_coverage="(&apos;2014-03-01&apos;, &apos;2014-04-01&apos;)" time="26.421" time_coverage="(&apos;2002-08-01&apos;, &apos;2014-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SEAICE" classname="test_data_support" data_type="SICONC" dataset="esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_25kmEASE2.2-1.NH" file="test_data_support.py" files="5138" line="94" name="test_open_local[remote_dataset13]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2012-09-03&apos;, &apos;2012-09-08&apos;)" time="7.592" time_coverage="(&apos;2002-06-01&apos;, &apos;2017-05-16&apos;)" time_frequency="day"></testcase><testcase cci_project="SEAICE" classname="test_data_support" data_type="SICONC" dataset="esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_25kmEASE2.2-1.SH" file="test_data_support.py" files="5138" line="94" name="test_open_local[remote_dataset14]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2008-06-07&apos;, &apos;2008-06-12&apos;)" time="9.053" time_coverage="(&apos;2002-06-01&apos;, &apos;2017-05-16&apos;)" time_frequency="day"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.SCIAMACHY.Envisat.WFMD.v4-0.r1" file="test_data_support.py" files="3209" line="94" name="test_open_local[remote_dataset15]" processing_level="L2" product_version="v4-0" sensor_id="SCIAMACHY" test_time_coverage="(&apos;2010-09-25&apos;, &apos;2011-01-09&apos;)" time="46.284" time_coverage="(&apos;2002-10-01&apos;, &apos;2011-12-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testg&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (corners_dim: 4, layer_dim: 20, level_dim: 21, sounding_dim: 4661)
Dimens... ENVISAT
    sensor:                    SCIAMACHY
    spatial_resolution:        30km x 60km at nadir (typically), ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AATSR.Envisat.AATSR.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset16]" processing_level="L3U" product_version="2-1" sensor_id="AATSR" test_time_coverage="(&apos;2003-10-20&apos;, &apos;2003-10-21&apos;)" time="28.648" time_coverage="(&apos;2002-07-24&apos;, &apos;2004-08-20&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset17]" processing_level="L3U" product_version="2-1" sensor_id="ATSR-2" test_time_coverage="(&apos;1996-10-15&apos;, &apos;1996-10-16&apos;)" time="77.495" time_coverage="(&apos;1995-08-01&apos;, &apos;1998-01-29&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.ATSR.ERS-1.ATSR1.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset18]" processing_level="L3U" product_version="2-1" sensor_id="ATSR" test_time_coverage="(&apos;1993-05-11&apos;, &apos;1993-05-12&apos;)" time="106.540" time_coverage="(&apos;1991-11-01&apos;, &apos;1994-02-21&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.r1" file="test_data_support.py" files="7912" line="94" name="test_open_local[remote_dataset19]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2002-10-23&apos;, &apos;2002-10-24&apos;)" time="21.300" time_coverage="(&apos;1998-09-24&apos;, &apos;2010-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.r1" file="test_data_support.py" files="7370" line="94" name="test_open_local[remote_dataset20]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2016-04-06&apos;, &apos;2016-04-07&apos;)" time="15.834" time_coverage="(&apos;2006-11-21&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.r1" file="test_data_support.py" files="5740" line="94" name="test_open_local[remote_dataset21]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2011-07-27&apos;, &apos;2011-07-28&apos;)" time="21.954" time_coverage="(&apos;2009-02-22&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.r1" file="test_data_support.py" files="5464" line="94" name="test_open_local[remote_dataset22]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2004-09-02&apos;, &apos;2004-09-02&apos;)" time="10.706" time_coverage="(&apos;2002-07-10&apos;, &apos;2010-01-01&apos;)" time_frequency="day"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testn&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                           (bnds: 2, lat: 3600, lon: 7200, time: 1)
Coord...ue-2-signed
    spatial_resolution:              0.05 degree
    creation_date:                   2019-01-31T14:49:36Z]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.r1" file="test_data_support.py" files="5278" line="94" name="test_open_local[remote_dataset23]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1993-05-14&apos;, &apos;1993-05-15&apos;)" time="18.279" time_coverage="(&apos;1991-09-16&apos;, &apos;1998-12-15&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.r1" file="test_data_support.py" files="4306" line="94" name="test_open_local[remote_dataset24]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1993-05-31&apos;, &apos;1993-05-31&apos;)" time="8.494" time_coverage="(&apos;1988-10-12&apos;, &apos;1994-09-14&apos;)" time_frequency="day"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testb&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                           (bnds: 2, lat: 3600, lon: 7200, time: 1)
Coord...ue-2-signed
    spatial_resolution:              0.05 degree
    creation_date:                   2019-01-31T13:06:26Z]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.r1" file="test_data_support.py" files="3616" line="94" name="test_open_local[remote_dataset25]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1996-03-25&apos;, &apos;1996-03-26&apos;)" time="18.903" time_coverage="(&apos;1995-01-19&apos;, &apos;2000-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.r1" file="test_data_support.py" files="3342" line="94" name="test_open_local[remote_dataset26]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2009-07-07&apos;, &apos;2009-07-08&apos;)" time="23.200" time_coverage="(&apos;2005-06-05&apos;, &apos;2010-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.r1" file="test_data_support.py" files="2846" line="94" name="test_open_local[remote_dataset27]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1988-03-31&apos;, &apos;1988-04-01&apos;)" time="22.306" time_coverage="(&apos;1985-01-04&apos;, &apos;1992-01-07&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.r1" file="test_data_support.py" files="2620" line="94" name="test_open_local[remote_dataset28]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2006-12-15&apos;, &apos;2006-12-15&apos;)" time="7.653" time_coverage="(&apos;2003-06-01&apos;, &apos;2007-01-01&apos;)" time_frequency="day"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testi&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                           (bnds: 2, lat: 3600, lon: 7200, time: 1)
Coord...ue-2-signed
    spatial_resolution:              0.05 degree
    creation_date:                   2019-01-31T14:34:51Z]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.r1" file="test_data_support.py" files="2498" line="94" name="test_open_local[remote_dataset29]" processing_level="L3C" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1982-01-23&apos;, &apos;1982-01-24&apos;)" time="18.220" time_coverage="(&apos;1981-08-24&apos;, &apos;1985-02-19&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTdepth" dataset="esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.anomaly" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset30]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;1983-10-12&apos;, &apos;1983-10-15&apos;)" time="6.377" time_coverage="(&apos;1981-09-01&apos;, &apos;2009-01-17&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTdepth" dataset="esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.sst" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset31]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;1998-11-09&apos;, &apos;1998-11-12&apos;)" time="5.362" time_coverage="(&apos;1981-09-01&apos;, &apos;2009-01-17&apos;)" time_frequency="day"></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMS" dataset="esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.04-5.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset32]" processing_level="L3S" product_version="04-5" sensor_id="multi-sensor" test_time_coverage="(&apos;2011-12-15&apos;, &apos;2012-02-06&apos;)" time="31.712" time_coverage="(&apos;1991-08-05&apos;, &apos;2018-12-21&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-30.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset33]" processing_level="L2P" product_version="2-30" sensor_id="ATSR-2" test_time_coverage="(&apos;1997-10-31&apos;, &apos;1997-11-07&apos;)" time="42.939" time_coverage="(&apos;1995-06-01&apos;, &apos;1998-02-03&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.SU.4-21.r1" file="test_data_support.py" files="3457" line="94" name="test_open_local[remote_dataset34]" processing_level="L3C" product_version="4-21" sensor_id="AATSR" test_time_coverage="(&apos;2007-02-27&apos;, &apos;2007-04-06&apos;)" time="31.396" time_coverage="(&apos;2002-07-24&apos;, &apos;2012-04-09&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.SU.4-21.r1" file="test_data_support.py" files="118" line="94" name="test_open_local[remote_dataset35]" processing_level="L3C" product_version="4-21" sensor_id="AATSR" test_time_coverage="(&apos;2006-03-01&apos;, &apos;2006-09-01&apos;)" time="8.499" time_coverage="(&apos;2002-07-01&apos;, &apos;2012-04-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="TC" dataset="esacci.OZONE.day.L3S.TC.multi-sensor.multi-platform.MERGED.fv0100.r1" file="test_data_support.py" files="185" line="94" name="test_open_local[remote_dataset36]" processing_level="L3S" product_version="fv0100" sensor_id="multi-sensor" test_time_coverage="(&apos;2005-05-01&apos;, &apos;2007-01-02&apos;)" time="15.370" time_coverage="(&apos;1996-03-10&apos;, &apos;2011-06-02&apos;)" time_frequency="day"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.Envisat.MERIS-AATSR.2-0.r1" file="test_data_support.py" files="108" line="94" name="test_open_local[remote_dataset37]" processing_level="L3C" product_version="2-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2004-11-01&apos;, &apos;2004-12-01&apos;)" time="13.137" time_coverage="(&apos;2003-01-01&apos;, &apos;2011-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.sinusoidal" file="test_data_support.py" files="1000" line="94" name="test_open_local[remote_dataset38]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2012-10-09&apos;, &apos;2012-10-09&apos;)" time="58.998" time_coverage="(&apos;2011-10-12&apos;, &apos;2014-07-08&apos;)" time_frequency="day"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.OCFP.v7-0-1.r1" file="test_data_support.py" files="2386" line="94" name="test_open_local[remote_dataset39]" processing_level="L2" product_version="v7-0-1" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2010-05-15&apos;, &apos;2010-08-18&apos;)" time="41.517" time_coverage="(&apos;2009-04-18&apos;, &apos;2015-12-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testg&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                           (m: 20, n: 989)
Dimensions without coordinates: m, n
Data vari...nsor:                    TANSO-FTS
    spatial_resolution:        10.5 km diameter footprint at nadir (typically), ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.OCFP.v2-1.r1" file="test_data_support.py" files="2387" line="94" name="test_open_local[remote_dataset40]" processing_level="L2" product_version="v2-1" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2014-05-15&apos;, &apos;2014-08-19&apos;)" time="44.895" time_coverage="(&apos;2009-04-18&apos;, &apos;2015-12-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testt&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                           (m: 20, n: 1018)
Dimensions without coordinates: m, n
Data var...nsor:                    TANSO-FTS
    spatial_resolution:        10.5 km diameter footprint at nadir (typically), ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="FIRE" classname="test_data_support" data_type="BA" dataset="esacci.FIRE.mon.L4.BA.MODIS.Terra.MODIS_TERRA.v5-1.r1" file="test_data_support.py" files="216" line="94" name="test_open_local[remote_dataset41]" processing_level="L4" product_version="v5-1" sensor_id="MODIS" test_time_coverage="(&apos;2010-08-01&apos;, &apos;2010-09-01&apos;)" time="11.863" time_coverage="(&apos;2001-01-01&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="7763" line="94" name="test_open_local[remote_dataset42]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;1999-03-08&apos;, &apos;1999-03-08&apos;)" time="2.493" time_coverage="(&apos;1997-09-04&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="1561" line="94" name="test_open_local[remote_dataset43]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-07-30&apos;, &apos;2010-08-04&apos;)" time="17.442" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.8-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="982" line="94" name="test_open_local[remote_dataset44]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2003-10-16&apos;, &apos;2003-10-24&apos;)" time="39.352" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.mon.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset45]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2001-12-01&apos;, &apos;2002-01-01&apos;)" time="38.279" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.AVHRR-AM.2-0.r1" file="test_data_support.py" files="279" line="94" name="test_open_local[remote_dataset46]" processing_level="L3C" product_version="2-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2003-10-01&apos;, &apos;2003-11-01&apos;)" time="16.101" time_coverage="(&apos;1991-10-01&apos;, &apos;2014-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-21.r1" file="test_data_support.py" files="89" line="94" name="test_open_local[remote_dataset47]" processing_level="L3C" product_version="4-21" sensor_id="ATSR-2" test_time_coverage="(&apos;1999-03-01&apos;, &apos;1999-10-01&apos;)" time="6.636" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.day.L3S.IOP.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="7032" line="94" name="test_open_local[remote_dataset48]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-09-29&apos;, &apos;2010-09-29&apos;)" time="378.172" time_coverage="(&apos;1997-09-04&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.5-days.L3S.IOP.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="1415" line="94" name="test_open_local[remote_dataset49]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2002-05-21&apos;, &apos;2002-05-26&apos;)" time="1257.260" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-31&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.8-days.L3S.IOP.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="890" line="94" name="test_open_local[remote_dataset50]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;1999-11-01&apos;, &apos;1999-11-09&apos;)" time="659.469" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-26&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.mon.L3S.IOP.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset51]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2004-02-01&apos;, &apos;2004-03-01&apos;)" time="1837.969" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.day.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="7032" line="94" name="test_open_local[remote_dataset52]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2002-06-10&apos;, &apos;2002-06-10&apos;)" time="159.124" time_coverage="(&apos;1997-09-04&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.5-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="1415" line="94" name="test_open_local[remote_dataset53]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2000-11-21&apos;, &apos;2000-11-26&apos;)" time="2343.445" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-31&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.8-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="890" line="94" name="test_open_local[remote_dataset54]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2004-04-30&apos;, &apos;2004-05-08&apos;)" time="5786.325" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-26&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.mon.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset55]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2016-10-01&apos;, &apos;2016-11-01&apos;)" time="2490.205" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.day.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.sinusoidal" file="test_data_support.py" files="618" line="94" name="test_open_local[remote_dataset56]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-12-20&apos;, &apos;2015-12-20&apos;)" time="191.072" time_coverage="(&apos;2015-04-24&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.mon.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.sinusoidal" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset57]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2005-12-01&apos;, &apos;2006-01-01&apos;)" time="523.068" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMS" dataset="esacci.SOILMOISTURE.day.L3S.SSMS.multi-sensor.multi-platform.ACTIVE.04-4.r1" file="test_data_support.py" files="9814" line="94" name="test_open_local[remote_dataset58]" processing_level="L3S" product_version="04-4" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-10-19&apos;, &apos;2010-12-03&apos;)" time="22.527" time_coverage="(&apos;1991-08-05&apos;, &apos;2018-07-01&apos;)" time_frequency="day"></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMS" dataset="esacci.SOILMOISTURE.day.L3S.SSMS.AMI-SCAT.multi-platform.ACTIVE.04-4.r1" file="test_data_support.py" files="13" line="94" name="test_open_local[remote_dataset59]" processing_level="L3S" product_version="04-4" sensor_id="AMI-SCAT" test_time_coverage="(&apos;2001-08-24&apos;, &apos;2001-10-29&apos;)" time="2.499" time_coverage="(&apos;2001-06-20&apos;, &apos;2001-10-29&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AATSR.Envisat.AATSR.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset60]" processing_level="L2P" product_version="2-1" sensor_id="AATSR" test_time_coverage="(&apos;2003-09-24&apos;, &apos;2003-09-24&apos;)" time="14.412" time_coverage="(&apos;2002-07-24&apos;, &apos;2005-06-14&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset61]" processing_level="L2P" product_version="2-1" sensor_id="ATSR-2" test_time_coverage="(&apos;1997-04-29&apos;, &apos;1997-04-29&apos;)" time="5.054" time_coverage="(&apos;1995-08-01&apos;, &apos;1999-02-07&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.ATSR.ERS-1.ATSR1.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset62]" processing_level="L2P" product_version="2-1" sensor_id="ATSR" test_time_coverage="(&apos;1994-07-18&apos;, &apos;1994-07-18&apos;)" time="15.916" time_coverage="(&apos;1991-11-01&apos;, &apos;1995-01-13&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTdepth" dataset="esacci.SST.climatology.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.2-1.r1" file="test_data_support.py" files="365" line="94" name="test_open_local[remote_dataset63]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;1996-10-14&apos;, &apos;1996-10-19&apos;)" time="6.114" time_coverage="(&apos;1996-01-01&apos;, &apos;1996-12-31&apos;)" time_frequency="climatology"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="7763" line="94" name="test_open_local[remote_dataset64]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;1997-12-24&apos;, &apos;1997-12-24&apos;)" time="2.853" time_coverage="(&apos;1997-09-04&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.5-days.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="1561" line="94" name="test_open_local[remote_dataset65]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-07-15&apos;, &apos;2010-07-20&apos;)" time="18.540" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.8-days.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="982" line="94" name="test_open_local[remote_dataset66]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2018-08-05&apos;, &apos;2018-08-13&apos;)" time="24.371" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.mon.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset67]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2017-05-01&apos;, &apos;2017-06-01&apos;)" time="23.667" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.ORAC.03-02.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset68]" processing_level="L2P" product_version="03-02" sensor_id="AATSR" test_time_coverage="(&apos;2004-05-05&apos;, &apos;2004-05-05&apos;)" time="6.311" time_coverage="(&apos;2002-07-24&apos;, &apos;2004-09-17&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.AATSR.03-02.r1" file="test_data_support.py" files="18" line="94" name="test_open_local[remote_dataset69]" processing_level="L2P" product_version="03-02" sensor_id="AATSR" test_time_coverage="(&apos;2004-07-16&apos;, &apos;2004-07-19&apos;)" time="9.419" time_coverage="(&apos;2004-07-10&apos;, &apos;2004-07-20&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.ATSR2-AATSR.2-0.r1" file="test_data_support.py" files="195" line="94" name="test_open_local[remote_dataset70]" processing_level="L3C" product_version="2-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2007-12-01&apos;, &apos;2008-01-01&apos;)" time="16.687" time_coverage="(&apos;1995-08-01&apos;, &apos;2012-04-01&apos;)" time_frequency="month"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.multi-sensor.multi-platform.AVHRR-PM.2-0.r1" file="test_data_support.py" files="392" line="94" name="test_open_local[remote_dataset71]" processing_level="L3C" product_version="2-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2006-11-01&apos;, &apos;2006-12-01&apos;)" time="15.704" time_coverage="(&apos;1982-01-01&apos;, &apos;2014-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.SRFP.v2-3-8.r1" file="test_data_support.py" files="1077" line="94" name="test_open_local[remote_dataset72]" processing_level="L2" product_version="v2-3-8" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2011-11-01&apos;, &apos;2012-04-07&apos;)" time="32.005" time_coverage="(&apos;2009-04-01&apos;, &apos;2015-12-27&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testz&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                                       (layer_dim: 12, level_dim: 13, pol...n:        10.5km x 10.5km at nadir (typically)
    _CoordSysBuilder:          ucar.nc2.dataset.conv.CF1Convention, ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.GOSAT.v2-3-8.r1" file="test_data_support.py" files="9" line="94" name="test_open_local[remote_dataset73]" processing_level="L2" product_version="v2-3-8" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2014-03-31&apos;, &apos;2014-09-01&apos;)" time="3.693" time_coverage="(&apos;2014-03-19&apos;, &apos;2014-09-01&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testf&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                                       (layer_dim: 12, level_dim: 13, pol...n:        10.5km x 10.5km at nadir (typically)
    _CoordSysBuilder:          ucar.nc2.dataset.conv.CF1Convention, ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.SCIAMACHY.Envisat.IMAP.v7-2.r1" file="test_data_support.py" files="1423" line="94" name="test_open_local[remote_dataset74]" processing_level="L2" product_version="v7-2" sensor_id="SCIAMACHY" test_time_coverage="(&apos;2003-10-18&apos;, &apos;2004-02-22&apos;)" time="28.324" time_coverage="(&apos;2003-01-11&apos;, &apos;2012-04-05&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testz&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 5597)
Dimensions without coo...sensor:                    SCIAMACHY
    history:                   15.05.2014 -  product generated with IMAP v72, ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.SRFP.v2-3-8.r1" file="test_data_support.py" files="1086" line="94" name="test_open_local[remote_dataset75]" processing_level="L2" product_version="v2-3-8" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2010-10-07&apos;, &apos;2011-03-19&apos;)" time="32.333" time_coverage="(&apos;2009-04-01&apos;, &apos;2015-12-27&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.teste&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                                       (layer_dim: 12, level_dim: 13, pol...n:        10.5km x 10.5km at nadir (typically)
    _CoordSysBuilder:          ucar.nc2.dataset.conv.CF1Convention, ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-30.r1" file="test_data_support.py" files="3432" line="94" name="test_open_local[remote_dataset76]" processing_level="L3C" product_version="2-30" sensor_id="AATSR" test_time_coverage="(&apos;2008-12-21&apos;, &apos;2009-01-05&apos;)" time="11.792" time_coverage="(&apos;2002-07-24&apos;, &apos;2012-04-09&apos;)" time_frequency="day"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testl&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                 (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitude       ...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-30.unc" file="test_data_support.py" files="239" line="94" name="test_open_local[remote_dataset77]" processing_level="L3C" product_version="2-30" sensor_id="AATSR" test_time_coverage="(&apos;2008-03-20&apos;, &apos;2008-03-21&apos;)" time="34.738" time_coverage="(&apos;2008-03-01&apos;, &apos;2009-01-01&apos;)" time_frequency="day"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testq&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                          (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitu...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-30.r1" file="test_data_support.py" files="117" line="94" name="test_open_local[remote_dataset78]" processing_level="L3C" product_version="2-30" sensor_id="AATSR" test_time_coverage="(&apos;2009-02-01&apos;, &apos;2009-11-01&apos;)" time="7.994" time_coverage="(&apos;2002-08-01&apos;, &apos;2012-04-01&apos;)" time_frequency="month"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testz&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                   (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitude     ...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.AATSR.Envisat.ADV.2-30.unc" file="test_data_support.py" files="4" line="94" name="test_open_local[remote_dataset79]" processing_level="L3C" product_version="2-30" sensor_id="AATSR" test_time_coverage="(&apos;2008-09-01&apos;, &apos;2008-12-01&apos;)" time="2.168" time_coverage="(&apos;2008-03-01&apos;, &apos;2008-12-01&apos;)" time_frequency="month"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testp&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                          (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitu...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AOD" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1" file="test_data_support.py" files="5260" line="94" name="test_open_local[remote_dataset80]" processing_level="L2P" product_version="2-2" sensor_id="MERIS" test_time_coverage="(&apos;2008-02-03&apos;, &apos;2008-02-04&apos;)" time="51.200" time_coverage="(&apos;2008-01-01&apos;, &apos;2008-12-31&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.SCIAMACHY.Envisat.BESD.v02-01-02.r1" file="test_data_support.py" files="3111" line="94" name="test_open_local[remote_dataset81]" processing_level="L2" product_version="v02-01-02" sensor_id="SCIAMACHY" test_time_coverage="(&apos;2005-08-14&apos;, &apos;2005-11-22&apos;)" time="31.005" time_coverage="(&apos;2003-01-08&apos;, &apos;2012-03-24&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testb&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 849)
Dimensions without coor...
    platform:                  Envisat
    sensor:                    SCIAMACHY
    spatial_resolution:         , ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMV" dataset="esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.04-4.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset82]" processing_level="L3S" product_version="04-4" sensor_id="multi-sensor" test_time_coverage="(&apos;2000-11-27&apos;, &apos;2001-01-27&apos;)" time="34.599" time_coverage="(&apos;1978-11-01&apos;, &apos;2006-03-19&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AATSR.Envisat.AATSR.2-1.day" file="test_data_support.py" files="3460" line="94" name="test_open_local[remote_dataset83]" processing_level="L3C" product_version="2-1" sensor_id="AATSR" test_time_coverage="(&apos;2008-07-06&apos;, &apos;2008-07-09&apos;)" time="45.105" time_coverage="(&apos;2002-07-24&apos;, &apos;2012-04-09&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.AATSR.Envisat.AATSR.2-1.night" file="test_data_support.py" files="3460" line="94" name="test_open_local[remote_dataset84]" processing_level="L3C" product_version="2-1" sensor_id="AATSR" test_time_coverage="(&apos;2006-08-31&apos;, &apos;2006-09-03&apos;)" time="51.055" time_coverage="(&apos;2002-07-24&apos;, &apos;2012-04-09&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.ATSR.ERS-1.ATSR1.2-1.r1" file="test_data_support.py" files="2826" line="94" name="test_open_local[remote_dataset85]" processing_level="L3C" product_version="2-1" sensor_id="ATSR" test_time_coverage="(&apos;1992-10-28&apos;, &apos;1992-10-30&apos;)" time="75.411" time_coverage="(&apos;1991-11-01&apos;, &apos;1996-01-11&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.day" file="test_data_support.py" files="2460" line="94" name="test_open_local[remote_dataset86]" processing_level="L3C" product_version="2-1" sensor_id="ATSR-2" test_time_coverage="(&apos;1999-11-19&apos;, &apos;1999-11-22&apos;)" time="50.855" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-23&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.day.L3C.SSTskin.ATSR-2.ERS-2.ATSR2.2-1.night" file="test_data_support.py" files="2460" line="94" name="test_open_local[remote_dataset87]" processing_level="L3C" product_version="2-1" sensor_id="ATSR-2" test_time_coverage="(&apos;2000-09-30&apos;, &apos;2000-10-03&apos;)" time="75.277" time_coverage="(&apos;1995-08-01&apos;, &apos;2003-06-23&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset88]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2008-04-20&apos;, &apos;2008-04-21&apos;)" time="7.287" time_coverage="(&apos;2006-11-21&apos;, &apos;2008-11-25&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset89]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2000-01-26&apos;, &apos;2000-01-26&apos;)" time="7.879" time_coverage="(&apos;1998-09-24&apos;, &apos;2001-02-28&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset90]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2010-05-20&apos;, &apos;2010-05-20&apos;)" time="7.866" time_coverage="(&apos;2009-02-22&apos;, &apos;2011-02-02&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset91]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2002-07-17&apos;, &apos;2002-07-17&apos;)" time="6.746" time_coverage="(&apos;2002-07-10&apos;, &apos;2004-06-13&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset92]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1993-04-21&apos;, &apos;1993-04-22&apos;)" time="7.077" time_coverage="(&apos;1991-09-16&apos;, &apos;1993-08-24&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset93]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1989-10-25&apos;, &apos;1989-10-25&apos;)" time="7.045" time_coverage="(&apos;1988-10-12&apos;, &apos;1990-11-06&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset94]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1996-09-10&apos;, &apos;1996-09-11&apos;)" time="6.759" time_coverage="(&apos;1995-01-19&apos;, &apos;1996-12-28&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset95]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2006-01-07&apos;, &apos;2006-01-07&apos;)" time="7.298" time_coverage="(&apos;2005-06-05&apos;, &apos;2007-05-17&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset96]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1985-07-26&apos;, &apos;1985-07-26&apos;)" time="6.353" time_coverage="(&apos;1985-01-04&apos;, &apos;1987-03-07&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset97]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2005-03-20&apos;, &apos;2005-03-20&apos;)" time="6.274" time_coverage="(&apos;2003-06-01&apos;, &apos;2005-05-17&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L3U.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset98]" processing_level="L3U" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1982-02-11&apos;, &apos;1982-02-11&apos;)" time="7.618" time_coverage="(&apos;1981-08-24&apos;, &apos;1983-10-01&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.day.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="7763" line="94" name="test_open_local[remote_dataset99]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2003-08-07&apos;, &apos;2003-08-07&apos;)" time="59.534" time_coverage="(&apos;1997-09-04&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.5-days.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="1561" line="94" name="test_open_local[remote_dataset100]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2014-04-06&apos;, &apos;2014-04-11&apos;)" time="172.181" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.8-days.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="982" line="94" name="test_open_local[remote_dataset101]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2003-01-25&apos;, &apos;2003-02-02&apos;)" time="295.394" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.mon.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset102]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2005-08-01&apos;, &apos;2005-09-01&apos;)" time="358.248" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.day.L3S.RRS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="7763" line="94" name="test_open_local[remote_dataset103]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;1997-12-23&apos;, &apos;1997-12-23&apos;)" time="15.276" time_coverage="(&apos;1997-09-04&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.5-days.L3S.RRS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="1561" line="94" name="test_open_local[remote_dataset104]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2000-03-31&apos;, &apos;2000-04-05&apos;)" time="110.490" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.8-days.L3S.RRS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="982" line="94" name="test_open_local[remote_dataset105]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2006-02-18&apos;, &apos;2006-02-26&apos;)" time="206.978" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-27&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.mon.L3S.RRS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset106]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2009-08-01&apos;, &apos;2009-09-01&apos;)" time="235.061" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMV" dataset="esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.04-5.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset107]" processing_level="L3S" product_version="04-5" sensor_id="multi-sensor" test_time_coverage="(&apos;1988-05-02&apos;, &apos;1988-07-07&apos;)" time="32.742" time_coverage="(&apos;1978-11-01&apos;, &apos;2006-03-19&apos;)" time_frequency="day"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.SMR.ODIN.MZM.v0001.r1" file="test_data_support.py" files="13" line="94" name="test_open_local[remote_dataset108]" processing_level="L3" product_version="v0001" sensor_id="SMR" test_time_coverage="(&apos;2008-01-01&apos;, &apos;2013-01-01&apos;)" time="1.940" time_coverage="(&apos;2001-01-01&apos;, &apos;2013-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AOD" dataset="esacci.AEROSOL.day.L3C.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1" file="test_data_support.py" files="366" line="94" name="test_open_local[remote_dataset109]" processing_level="L3C" product_version="2-2" sensor_id="MERIS" test_time_coverage="(&apos;2008-01-01&apos;, &apos;2008-03-15&apos;)" time="35.762" time_coverage="(&apos;2008-01-01&apos;, &apos;2009-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AOD" dataset="esacci.AEROSOL.mon.L3C.AOD.MERIS.Envisat.MERIS_ENVISAT.2-2.r1" file="test_data_support.py" files="12" line="94" name="test_open_local[remote_dataset110]" processing_level="L3C" product_version="2-2" sensor_id="MERIS" test_time_coverage="(&apos;2008-10-01&apos;, &apos;2008-12-01&apos;)" time="2.245" time_coverage="(&apos;2008-01-01&apos;, &apos;2008-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="CLOUD" classname="test_data_support" data_type="CLD_PRODUCTS" dataset="esacci.CLOUD.mon.L3C.CLD_PRODUCTS.MODIS.Terra.MODIS_TERRA.2-0.r1" file="test_data_support.py" files="179" line="94" name="test_open_local[remote_dataset111]" processing_level="L3C" product_version="2-0" sensor_id="MODIS" test_time_coverage="(&apos;2008-03-01&apos;, &apos;2008-04-01&apos;)" time="18.942" time_coverage="(&apos;2000-02-01&apos;, &apos;2014-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="7032" line="94" name="test_open_local[remote_dataset112]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-09-27&apos;, &apos;2010-09-27&apos;)" time="5.046" time_coverage="(&apos;1997-09-04&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.5-days.L3S.K_490.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="1415" line="94" name="test_open_local[remote_dataset113]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2013-03-27&apos;, &apos;2013-04-01&apos;)" time="16.109" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-31&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.8-days.L3S.K_490.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="890" line="94" name="test_open_local[remote_dataset114]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;1998-06-18&apos;, &apos;1998-06-26&apos;)" time="12.989" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-26&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.mon.L3S.K_490.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset115]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-02-01&apos;, &apos;2015-03-01&apos;)" time="30.211" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.day.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="7032" line="94" name="test_open_local[remote_dataset116]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2012-04-12&apos;, &apos;2012-04-12&apos;)" time="178.751" time_coverage="(&apos;1997-09-04&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.5-days.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="1415" line="94" name="test_open_local[remote_dataset117]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2002-06-15&apos;, &apos;2002-06-20&apos;)" time="544.432" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-31&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.8-days.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="890" line="94" name="test_open_local[remote_dataset118]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2013-02-10&apos;, &apos;2013-02-18&apos;)" time="828.527" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-26&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="RRS" dataset="esacci.OC.mon.L3S.RRS.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset119]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-11-01&apos;, &apos;2015-12-01&apos;)" time="973.737" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.SCIAMACHY.Envisat.WFMD.v4-0.r1" file="test_data_support.py" files="3299" line="94" name="test_open_local[remote_dataset120]" processing_level="L2" product_version="v4-0" sensor_id="SCIAMACHY" test_time_coverage="(&apos;2006-03-14&apos;, &apos;2006-06-28&apos;)" time="32.605" time_coverage="(&apos;2002-10-01&apos;, &apos;2012-04-08&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testf&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (corners_dim: 4, layer_dim: 20, level_dim: 21, sounding_dim: 1278)
Dimens... ENVISAT
    sensor:                    SCIAMACHY
    spatial_resolution:        30km x 60km at nadir (typically), ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.SRPR.v2-3-8.r1" file="test_data_support.py" files="1155" line="94" name="test_open_local[remote_dataset121]" processing_level="L2" product_version="v2-3-8" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2010-08-05&apos;, &apos;2010-10-31&apos;)" time="22.991" time_coverage="(&apos;2009-04-08&apos;, &apos;2015-12-29&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testb&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                                                       (layer_dim: 4, level_dim: 5, polar...n:        10.5km x 10.5km at nadir (typically)
    _CoordSysBuilder:          ucar.nc2.dataset.conv.CF1Convention, ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMV" dataset="esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.COMBINED.04-4.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset122]" processing_level="L3S" product_version="04-4" sensor_id="multi-sensor" test_time_coverage="(&apos;1984-08-28&apos;, &apos;1984-12-07&apos;)" time="35.173" time_coverage="(&apos;1978-11-01&apos;, &apos;2006-03-19&apos;)" time_frequency="day"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.Metop-A.AVHRRMTA_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset123]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2007-11-21&apos;, &apos;2007-11-21&apos;)" time="5.344" time_coverage="(&apos;2006-11-21&apos;, &apos;2009-02-04&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {11808, 11963}">local_dataset = &apos;local.testw&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {11808, 11963}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-15.AVHRR15_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset124]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;1999-01-25&apos;, &apos;1999-01-25&apos;)" time="4.226" time_coverage="(&apos;1998-09-24&apos;, &apos;2001-05-10&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {13081, 13114, 13094}">local_dataset = &apos;local.testg&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {13081, 13114, 13094}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-12.AVHRR12_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset125]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1992-03-14&apos;, &apos;1992-03-14&apos;)" time="4.071" time_coverage="(&apos;1991-09-16&apos;, &apos;1993-08-25&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {11946, 8930, 13114}">local_dataset = &apos;local.testa&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {11946, 8930, 13114}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-19.AVHRR19_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset126]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2011-07-08&apos;, &apos;2011-07-08&apos;)" time="3.390" time_coverage="(&apos;2009-03-22&apos;, &apos;2011-09-29&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12080, 12098, 13114}">local_dataset = &apos;local.testj&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12080, 12098, 13114}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-11.AVHRR11_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset127]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1989-05-22&apos;, &apos;1989-05-22&apos;)" time="3.482" time_coverage="(&apos;1988-10-12&apos;, &apos;1990-11-06&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12602, 12238, 13102}">local_dataset = &apos;local.testt&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12602, 12238, 13102}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-14.AVHRR14_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset128]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1996-05-18&apos;, &apos;1996-05-18&apos;)" time="3.512" time_coverage="(&apos;1995-01-19&apos;, &apos;1996-12-28&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12312, 13108, 12318}">local_dataset = &apos;local.testm&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12312, 13108, 12318}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-17.AVHRR17_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset129]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2003-02-05&apos;, &apos;2003-02-05&apos;)" time="3.830" time_coverage="(&apos;2002-07-10&apos;, &apos;2004-12-03&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {13118, 13102, 11782}">local_dataset = &apos;local.testz&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {13118, 13102, 11782}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-9.AVHRR09_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset130]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1986-04-04&apos;, &apos;1986-04-04&apos;)" time="3.240" time_coverage="(&apos;1985-01-04&apos;, &apos;1987-03-07&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12130, 12068, 10583}">local_dataset = &apos;local.testp&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12130, 12068, 10583}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-2.NOAA-7.AVHRR07_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset131]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-2" test_time_coverage="(&apos;1982-06-26&apos;, &apos;1982-06-26&apos;)" time="4.159" time_coverage="(&apos;1981-08-24&apos;, &apos;1983-10-01&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12096, 10236, 7765, 13006}">local_dataset = &apos;local.testa&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12096, 10236, 7765, 13006}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-16.AVHRR16_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset132]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2004-11-03&apos;, &apos;2004-11-04&apos;)" time="3.717" time_coverage="(&apos;2003-06-01&apos;, &apos;2005-05-14&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12162, 12411, 13140}">local_dataset = &apos;local.testx&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12162, 12411, 13140}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTskin" dataset="esacci.SST.satellite-orbit-frequency.L2P.SSTskin.AVHRR-3.NOAA-18.AVHRR18_G.2-1.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset133]" processing_level="L2P" product_version="2-1" sensor_id="AVHRR-3" test_time_coverage="(&apos;2005-09-04&apos;, &apos;2005-09-04&apos;)" time="4.252" time_coverage="(&apos;2005-06-05&apos;, &apos;2008-05-26&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12270, 13110}">local_dataset = &apos;local.testa&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:680: in combine_by_coords
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:198: in _combine_nd
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:220: in _combine_all_along_first_dim
    datasets, dim, compat, data_vars, coords, fill_value, join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:248: in _combine_1d
    join=join,
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:133: in concat
    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/concat.py:301: in _dataset_concat
    *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

join = &apos;outer&apos;, copy = False, indexes = {}, exclude = [&apos;time&apos;]

    def align(
        *objects,
        join=&quot;inner&quot;,
        copy=True,
        indexes=None,
        exclude=frozenset(),
        fill_value=dtypes.NA,
    ):
        &quot;&quot;&quot;
        Given any number of Dataset and/or DataArray objects, returns new
        objects with aligned indexes and dimension sizes.
    
        Array from the aligned objects are suitable as input to mathematical
        operators, because along each dimension they have the same index and size.
    
        Missing values (if ``join != &apos;inner&apos;``) are filled with ``fill_value``.
        The default fill value is NaN.
    
        Parameters
        ----------
        *objects : Dataset or DataArray
            Objects to align.
        join : {&apos;outer&apos;, &apos;inner&apos;, &apos;left&apos;, &apos;right&apos;, &apos;exact&apos;, &apos;override&apos;}, optional
            Method for joining the indexes of the passed objects along each
            dimension:
    
            - &apos;outer&apos;: use the union of object indexes
            - &apos;inner&apos;: use the intersection of object indexes
            - &apos;left&apos;: use indexes from the first object with each dimension
            - &apos;right&apos;: use indexes from the last object with each dimension
            - &apos;exact&apos;: instead of aligning, raise `ValueError` when indexes to be
              aligned are not equal
            - &apos;override&apos;: if indexes are of same size, rewrite indexes to be
              those of the first object with that dimension. Indexes for the same
              dimension must have the same size in all objects.
        copy : bool, optional
            If ``copy=True``, data in the return values is always copied. If
            ``copy=False`` and reindexing is unnecessary, or can be performed with
            only slice operations, then the output may share memory with the input.
            In either case, new xarray objects are always returned.
        indexes : dict-like, optional
            Any indexes explicitly provided with the `indexes` argument should be
            used in preference to the aligned indexes.
        exclude : sequence of str, optional
            Dimensions that must be excluded from alignment
        fill_value : scalar, optional
            Value to use for newly missing values
    
        Returns
        -------
        aligned : same as `*objects`
            Tuple of objects with aligned coordinates.
    
        Raises
        ------
        ValueError
            If any dimensions without labels on the arguments have different sizes,
            or a different size than the size of the aligned dimension labels.
    
        Examples
        --------
    
        &gt;&gt;&gt; import xarray as xr
        &gt;&gt;&gt; x = xr.DataArray([[25, 35], [10, 24]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 40.], &apos;lon&apos;: [100., 120.]})
        &gt;&gt;&gt; y = xr.DataArray([[20, 5], [7, 13]], dims=(&apos;lat&apos;, &apos;lon&apos;),
        ...              coords={&apos;lat&apos;: [35., 42.], &apos;lon&apos;: [100., 120.]})
    
        &gt;&gt;&gt; x
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; y
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[25, 35]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 1, lon: 2)&gt;
        array([[20,  5]])
        Coordinates:
        * lat      (lat) float64 35.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[25., 35.],
               [10., 24.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan],
               [ 7., 13.]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;outer&apos;, fill_value=-999)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  25,   35],
               [  10,   24],
               [-999, -999]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 3, lon: 2)&gt;
        array([[  20,    5],
               [-999, -999],
               [   7,   13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;left&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20.,  5.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;right&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25., 35.],
               [nan, nan]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 42.0
        * lon      (lon) float64 100.0 120.0
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;exact&apos;)
        Traceback (most recent call last):
        ...
            &quot;indexes along dimension {!r} are not equal&quot;.format(dim)
        ValueError: indexes along dimension &apos;lat&apos; are not equal
    
        &gt;&gt;&gt; a, b = xr.align(x, y, join=&apos;override&apos;)
        &gt;&gt;&gt; a
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[25, 35],
               [10, 24]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
        &gt;&gt;&gt; b
        &lt;xarray.DataArray (lat: 2, lon: 2)&gt;
        array([[20,  5],
               [ 7, 13]])
        Coordinates:
        * lat      (lat) float64 35.0 40.0
        * lon      (lon) float64 100.0 120.0
    
        &quot;&quot;&quot;
        if indexes is None:
            indexes = {}
    
        if not indexes and len(objects) == 1:
            # fast path for the trivial case
            (obj,) = objects
            return (obj.copy(deep=copy),)
    
        all_indexes = defaultdict(list)
        unlabeled_dim_sizes = defaultdict(set)
        for obj in objects:
            for dim in obj.dims:
                if dim not in exclude:
                    try:
                        index = obj.indexes[dim]
                    except KeyError:
                        unlabeled_dim_sizes[dim].add(obj.sizes[dim])
                    else:
                        all_indexes[dim].append(index)
    
        if join == &quot;override&quot;:
            objects = _override_indexes(objects, all_indexes, exclude)
    
        # We don&apos;t reindex over dimensions with all equal indexes for two reasons:
        # - It&apos;s faster for the usual case (already aligned objects).
        # - It ensures it&apos;s possible to do operations that don&apos;t require alignment
        #   on indexes with duplicate values (which cannot be reindexed with
        #   pandas). This is useful, e.g., for overwriting such duplicate indexes.
        joiner = _get_joiner(join)
        joined_indexes = {}
        for dim, matching_indexes in all_indexes.items():
            if dim in indexes:
                index = utils.safe_cast_to_index(indexes[dim])
                if (
                    any(not index.equals(other) for other in matching_indexes)
                    or dim in unlabeled_dim_sizes
                ):
                    joined_indexes[dim] = index
            else:
                if (
                    any(
                        not matching_indexes[0].equals(other)
                        for other in matching_indexes[1:]
                    )
                    or dim in unlabeled_dim_sizes
                ):
                    if join == &quot;exact&quot;:
                        raise ValueError(f&quot;indexes along dimension {dim!r} are not equal&quot;)
                    index = joiner(matching_indexes)
                    joined_indexes[dim] = index
                else:
                    index = matching_indexes[0]
    
            if dim in unlabeled_dim_sizes:
                unlabeled_sizes = unlabeled_dim_sizes[dim]
                labeled_size = index.size
                if len(unlabeled_sizes | {labeled_size}) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension size(s) %r &quot;
                        &quot;than the size of the aligned dimension labels: %r&quot;
                        % (dim, unlabeled_sizes, labeled_size)
                    )
    
        for dim in unlabeled_dim_sizes:
            if dim not in all_indexes:
                sizes = unlabeled_dim_sizes[dim]
                if len(sizes) &gt; 1:
                    raise ValueError(
                        &quot;arguments without labels along dimension %r cannot be &quot;
                        &quot;aligned because they have different dimension sizes: %r&quot;
&gt;                       % (dim, sizes)
                    )
E                   ValueError: arguments without labels along dimension &apos;nj&apos; cannot be aligned because they have different dimension sizes: {12270, 13110}

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/alignment.py:321: ValueError</failure></testcase><testcase cci_project="LC" classname="test_data_support" data_type="LCCS" dataset="esacci.LC.yr.L4.LCCS.multi-sensor.multi-platform.Map.2-0-7.r1" file="test_data_support.py" files="24" line="94" name="test_open_local[remote_dataset134]" processing_level="L4" product_version="2-0-7" sensor_id="multi-sensor" test_time_coverage="(&apos;2004-01-01&apos;, &apos;2005-01-01&apos;)" time="296.609" time_coverage="(&apos;1992-01-01&apos;, &apos;2015-01-01&apos;)" time_frequency="year"></testcase><testcase cci_project="SEASURFACESALINITY" classname="test_data_support" data_type="SSS" dataset="esacci.SEASURFACESALINITY.day.L4.SSS.multi-sensor.multi-platform.MERGED_OI_7DAY_RUNNINGMEAN_DAILY_25km.01-08.r1" file="test_data_support.py" files="3222" line="94" name="test_open_local[remote_dataset135]" processing_level="L4" product_version="01-08" sensor_id="multi-sensor" test_time_coverage="(&apos;2018-08-10&apos;, &apos;2018-08-15&apos;)" time="5.576" time_coverage="(&apos;2010-01-06&apos;, &apos;2018-11-02&apos;)" time_frequency="day"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.SCIAMACHY.Envisat.SCIAMACHY_ENVISAT.v0001.r1" file="test_data_support.py" files="10" line="94" name="test_open_local[remote_dataset136]" processing_level="L3" product_version="v0001" sensor_id="SCIAMACHY" test_time_coverage="(&apos;2003-01-01&apos;, &apos;2011-01-01&apos;)" time="2.515" time_coverage="(&apos;2002-01-01&apos;, &apos;2011-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AEX" dataset="esacci.AEROSOL.5-days.L3C.AEX.GOMOS.Envisat.AERGOM.2-19.r1" file="test_data_support.py" files="737" line="94" name="test_open_local[remote_dataset137]" processing_level="L3C" product_version="2-19" sensor_id="GOMOS" test_time_coverage="(&apos;2005-09-17&apos;, &apos;2006-11-26&apos;)" time="34.625" time_coverage="(&apos;2002-04-01&apos;, &apos;2012-04-28&apos;)" time_frequency="5 days"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testi&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                     (altitude: 43, latitude: 36, longitude: 6)
Coordinates:
  * altitude....0
    endDate:          53659.0
    inputFileList:    aergom_R18908_S0143_20051012_000202.ncaergom_R18908_S0..., ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.ORAC.03-02.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset138]" processing_level="L2P" product_version="03-02" sensor_id="ATSR-2" test_time_coverage="(&apos;1997-12-16&apos;, &apos;1997-12-17&apos;)" time="9.364" time_coverage="(&apos;1995-08-01&apos;, &apos;1998-03-15&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.ATSR2.04-01.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset139]" processing_level="L2P" product_version="04-01" sensor_id="ATSR-2" test_time_coverage="(&apos;2000-03-02&apos;, &apos;2000-03-05&apos;)" time="13.566" time_coverage="(&apos;1995-06-01&apos;, &apos;2000-08-15&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.day.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-30.r1" file="test_data_support.py" files="2479" line="94" name="test_open_local[remote_dataset140]" processing_level="L3C" product_version="2-30" sensor_id="ATSR-2" test_time_coverage="(&apos;2000-08-15&apos;, &apos;2000-08-30&apos;)" time="11.583" time_coverage="(&apos;1995-06-01&apos;, &apos;2003-05-01&apos;)" time_frequency="day"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testk&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                 (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitude       ...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.mon.L3C.AER_PRODUCTS.ATSR-2.ERS-2.ADV.2-30.r1" file="test_data_support.py" files="85" line="94" name="test_open_local[remote_dataset141]" processing_level="L3C" product_version="2-30" sensor_id="ATSR-2" test_time_coverage="(&apos;1995-06-01&apos;, &apos;1996-09-01&apos;)" time="8.203" time_coverage="(&apos;1995-06-01&apos;, &apos;2002-12-01&apos;)" time_frequency="month"><failure message="TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;">local_dataset = &apos;local.testc&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in open_mfdataset
    datasets = [preprocess(ds) for ds in datasets]
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:903: in &lt;listcomp&gt;
    datasets = [preprocess(ds) for ds in datasets]
../../../cate/cate/core/ds.py:741: in preprocess
    norm_ds = normalize_missing_time(normalize_coord_vars(raw_ds))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ds = &lt;xarray.Dataset&gt;
Dimensions:                   (latitude: 180, longitude: 360, time: 1)
Coordinates:
  * latitude     ...CDF Climate and Forecast (CF) Metadata Conv...
    license:                   ESA CCI Data Policy: free and open access

    def normalize_missing_time(ds: xr.Dataset) -&gt; xr.Dataset:
        &quot;&quot;&quot;
        Add a time coordinate variable and their associated bounds coordinate variables
        if temporal CF attributes ``time_coverage_start`` and ``time_coverage_end``
        are given but the time dimension is missing.
    
        The new time coordinate variable will be named ``time`` with dimension [&apos;time&apos;] and shape [1].
        The time bounds coordinates variable will be named ``time_bnds`` with dimensions [&apos;time&apos;, &apos;bnds&apos;] and shape [1,2].
        Both are of data type ``datetime64``.
    
        :param ds: Dataset to adjust
        :return: Adjusted dataset
        &quot;&quot;&quot;
        time_coverage_start = ds.attrs.get(&apos;time_coverage_start&apos;)
        if time_coverage_start is not None:
            # noinspection PyBroadException
            try:
                time_coverage_start = pd.to_datetime(time_coverage_start)
            except BaseException:
                pass
    
        time_coverage_end = ds.attrs.get(&apos;time_coverage_end&apos;)
        if time_coverage_end is not None:
            # noinspection PyBroadException
            try:
                time_coverage_end = pd.to_datetime(time_coverage_end)
            except BaseException:
                pass
    
        if not time_coverage_start and not time_coverage_end:
            # Can&apos;t do anything
            return ds
    
        if &apos;time&apos; in ds:
            time = ds.time
            if not time.dims:
                ds = ds.drop_vars(&apos;time&apos;)
            elif len(time.dims) == 1:
                time_dim_name = time.dims[0]
                is_time_used_as_dim = any([(time_dim_name in ds[var_name].dims) for var_name in ds.data_vars])
                if is_time_used_as_dim:
                    # It seems we already have valid time coordinates
                    return ds
                time_bnds_var_name = time.attrs.get(&apos;bounds&apos;)
                if time_bnds_var_name in ds:
                    ds = ds.drop_vars(time_bnds_var_name)
                ds = ds.drop_vars(&apos;time&apos;)
                ds = ds.drop_vars([var_name for var_name in ds.coords if time_dim_name in ds.coords[var_name].dims])
    
        if time_coverage_start or time_coverage_end:
            # noinspection PyBroadException
            try:
                ds = ds.expand_dims(&apos;time&apos;)
            except BaseException as e:
                warnings.warn(f&apos;failed to add time dimension: {e}&apos;)
    
            if time_coverage_start and time_coverage_end:
&gt;               time_value = time_coverage_start + 0.5 * (time_coverage_end - time_coverage_start)
E               TypeError: unsupported operand type(s) for -: &apos;Timestamp&apos; and &apos;str&apos;

../../../cate/cate/core/opimpl.py:359: TypeError</failure></testcase><testcase cci_project="LC" classname="test_data_support" data_type="WB" dataset="esacci.LC.13-yrs.L4.WB.ASAR.Envisat.Map.4-0.r1" file="test_data_support.py" files="1" line="94" name="test_open_local[remote_dataset142]" processing_level="L4" product_version="4-0" sensor_id="ASAR" test_time_coverage="(&apos;2000-01-01&apos;, &apos;2000-01-01&apos;)" time="5.589" time_coverage="(&apos;2000-01-01&apos;, &apos;2000-01-01&apos;)" time_frequency="13 years"></testcase><testcase cci_project="SST" classname="test_data_support" data_type="SSTdepth" dataset="esacci.SST.day.L4.SSTdepth.multi-sensor.multi-platform.OSTIA.1-1.r1" file="test_data_support.py" files="7062" line="94" name="test_open_local[remote_dataset143]" processing_level="L4" product_version="1-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2005-07-18&apos;, &apos;2005-07-21&apos;)" time="5.000" time_coverage="(&apos;1991-09-01&apos;, &apos;2011-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.SMR.ODIN.SMR_ODIN.v0001.r1" file="test_data_support.py" files="12" line="94" name="test_open_local[remote_dataset144]" processing_level="L3" product_version="v0001" sensor_id="SMR" test_time_coverage="(&apos;2002-01-01&apos;, &apos;2012-01-01&apos;)" time="3.295" time_coverage="(&apos;2001-01-01&apos;, &apos;2012-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.OCPR.v7-0.r1" file="test_data_support.py" files="2393" line="94" name="test_open_local[remote_dataset145]" processing_level="L2" product_version="v7-0" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2015-03-22&apos;, &apos;2015-05-01&apos;)" time="24.416" time_coverage="(&apos;2009-04-18&apos;, &apos;2015-12-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testo&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                           (m: 20, n: 2070)
Dimensions without coordinates: m, n
Data var...OSAT
    sensor:                    TANSO-FTS
    spatial_resolution:        10.5km x 10.5km at nadir (typically), ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="2307" line="94" name="test_open_local[remote_dataset146]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-10-12&apos;, &apos;2015-10-12&apos;)" time="6.205" time_coverage="(&apos;2012-09-07&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.mon.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset147]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;1999-09-01&apos;, &apos;1999-10-01&apos;)" time="27.464" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SOILMOISTURE" classname="test_data_support" data_type="SSMV" dataset="esacci.SOILMOISTURE.day.L3S.SSMV.multi-sensor.multi-platform.PASSIVE.04-5.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset148]" processing_level="L3S" product_version="04-5" sensor_id="multi-sensor" test_time_coverage="(&apos;2000-03-05&apos;, &apos;2000-04-30&apos;)" time="27.415" time_coverage="(&apos;1978-11-01&apos;, &apos;2006-03-19&apos;)" time_frequency="day"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.SU.4-21.r1" file="test_data_support.py" files="10000" line="94" name="test_open_local[remote_dataset149]" processing_level="L2P" product_version="4-21" sensor_id="AATSR" test_time_coverage="(&apos;2003-04-26&apos;, &apos;2003-04-29&apos;)" time="20.016" time_coverage="(&apos;2002-07-24&apos;, &apos;2004-08-31&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.ATSR-2.ERS-2.SU.4-21.r1" file="test_data_support.py" files="42" line="94" name="test_open_local[remote_dataset150]" processing_level="L2P" product_version="4-21" sensor_id="ATSR-2" test_time_coverage="(&apos;2002-08-12&apos;, &apos;2002-08-14&apos;)" time="11.295" time_coverage="(&apos;2002-08-11&apos;, &apos;2002-08-14&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.SU.4-21u.r1" file="test_data_support.py" files="28" line="94" name="test_open_local[remote_dataset151]" processing_level="L2P" product_version="4-21u" sensor_id="AATSR" test_time_coverage="(&apos;2008-06-01&apos;, &apos;2008-06-02&apos;)" time="16.399" time_coverage="(&apos;2008-06-01&apos;, &apos;2008-06-02&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="AEROSOL" classname="test_data_support" data_type="AER_PRODUCTS" dataset="esacci.AEROSOL.satellite-orbit-frequency.L2P.AER_PRODUCTS.AATSR.Envisat.AATSR_ENVISAT.4-21.r1" file="test_data_support.py" files="1" line="94" name="test_open_local[remote_dataset152]" processing_level="L2P" product_version="4-21" sensor_id="AATSR" test_time_coverage="(&apos;2005-01-01&apos;, &apos;2005-01-01&apos;)" time="0.697" time_coverage="(&apos;2005-01-01&apos;, &apos;2005-01-01&apos;)" time_frequency="satellite-orbit-frequency"></testcase><testcase cci_project="OZONE" classname="test_data_support" data_type="LP" dataset="esacci.OZONE.mon.L3.LP.MIPAS.Envisat.MIPAS_ENVISAT.v0001.r1" file="test_data_support.py" files="7" line="94" name="test_open_local[remote_dataset153]" processing_level="L3" product_version="v0001" sensor_id="MIPAS" test_time_coverage="(&apos;2010-01-01&apos;, &apos;2011-01-01&apos;)" time="0.685" time_coverage="(&apos;2005-01-01&apos;, &apos;2011-01-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SEAICE" classname="test_data_support" data_type="SICONC" dataset="esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_50kmEASE2.2-1.NH" file="test_data_support.py" files="5137" line="94" name="test_open_local[remote_dataset154]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2017-01-03&apos;, &apos;2017-01-11&apos;)" time="12.281" time_coverage="(&apos;2002-06-01&apos;, &apos;2017-05-16&apos;)" time_frequency="day"></testcase><testcase cci_project="SEAICE" classname="test_data_support" data_type="SICONC" dataset="esacci.SEAICE.day.L4.SICONC.multi-sensor.multi-platform.AMSR_50kmEASE2.2-1.SH" file="test_data_support.py" files="5137" line="94" name="test_open_local[remote_dataset155]" processing_level="L4" product_version="2-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-06-03&apos;, &apos;2015-06-11&apos;)" time="7.442" time_coverage="(&apos;2002-06-01&apos;, &apos;2017-05-16&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.day.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="7032" line="94" name="test_open_local[remote_dataset156]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2005-08-23&apos;, &apos;2005-08-23&apos;)" time="4.915" time_coverage="(&apos;1997-09-04&apos;, &apos;2017-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.5-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="1415" line="94" name="test_open_local[remote_dataset157]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-12-17&apos;, &apos;2015-12-22&apos;)" time="12.138" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-31&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.8-days.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="890" line="94" name="test_open_local[remote_dataset158]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2001-08-13&apos;, &apos;2001-08-21&apos;)" time="16.222" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-26&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="CHLOR_A" dataset="esacci.OC.mon.L3S.CHLOR_A.multi-sensor.multi-platform.MERGED.3-1.geographic" file="test_data_support.py" files="232" line="94" name="test_open_local[remote_dataset159]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-04-01&apos;, &apos;2015-05-01&apos;)" time="22.151" time_coverage="(&apos;1997-09-04&apos;, &apos;2016-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.3-1.sinusoidal" file="test_data_support.py" files="1000" line="94" name="test_open_local[remote_dataset160]" processing_level="L3S" product_version="3-1" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-11-10&apos;, &apos;2015-11-10&apos;)" time="69.939" time_coverage="(&apos;2014-02-18&apos;, &apos;2016-11-14&apos;)" time_frequency="day"></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.TANSO-FTS.GOSAT.EMMA.v2-2c.r1" file="test_data_support.py" files="1816" line="94" name="test_open_local[remote_dataset161]" processing_level="L2" product_version="v2-2c" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2013-11-28&apos;, &apos;2014-03-08&apos;)" time="19.469" time_coverage="(&apos;2009-06-01&apos;, &apos;2014-05-30&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testo&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 92)
Dimensions without coord...ss
    platform:                  GOSAT
    sensor:                    TANSO-FTS
    spatial_resolution:         , ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.multi-sensor.multi-platform.EMMA.v2-2a.r1" file="test_data_support.py" files="1035" line="94" name="test_open_local[remote_dataset162]" processing_level="L2" product_version="v2-2a" sensor_id="multi-sensor" test_time_coverage="(&apos;2009-07-15&apos;, &apos;2009-10-23&apos;)" time="23.428" time_coverage="(&apos;2009-06-01&apos;, &apos;2012-03-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testt&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 393)
Dimensions without coor...              Envisat, GOSAT
    sensor:                    SCIAMACHY, TANSO-FTS
    spatial_resolution:         , ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CO2" dataset="esacci.GHG.satellite-orbit-frequency.L2.CO2.multi-sensor.multi-platform.EMMA.v2-2b.r1" file="test_data_support.py" files="1034" line="94" name="test_open_local[remote_dataset163]" processing_level="L2" product_version="v2-2b" sensor_id="multi-sensor" test_time_coverage="(&apos;2010-01-04&apos;, &apos;2010-04-14&apos;)" time="18.003" time_coverage="(&apos;2009-06-01&apos;, &apos;2012-03-31&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testg&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 150)
Dimensions without coor...              Envisat, GOSAT
    sensor:                    SCIAMACHY, TANSO-FTS
    spatial_resolution:         , ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="GHG" classname="test_data_support" data_type="CH4" dataset="esacci.GHG.satellite-orbit-frequency.L2.CH4.TANSO-FTS.GOSAT.EMMA.ch4_v1-2.r1" file="test_data_support.py" files="1816" line="94" name="test_open_local[remote_dataset164]" processing_level="L2" product_version="ch4_v1-2" sensor_id="TANSO-FTS" test_time_coverage="(&apos;2011-07-09&apos;, &apos;2011-10-17&apos;)" time="26.174" time_coverage="(&apos;2009-06-01&apos;, &apos;2014-05-30&apos;)" time_frequency="satellite-orbit-frequency"><failure message="ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation">local_dataset = &apos;local.testx&apos;

    def test_open_local(local_dataset):
&gt;       ds.open_dataset(local_dataset)

test_data_support.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../../cate/cate/core/ds.py:678: in open_dataset
    return data_source.open_dataset(time_range, region, var_names, monitor=monitor)
../../../cate/cate/ds/local.py:187: in open_dataset
    monitor=monitor)
../../../cate/cate/core/ds.py:764: in open_xarray_dataset
    **kwargs)
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/backends/api.py:951: in open_mfdataset
    datasets, compat=compat, data_vars=data_vars, coords=coords, join=join
/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:667: in combine_by_coords
    list(datasets_with_same_vars)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

datasets = [&lt;xarray.Dataset&gt;
Dimensions:                (layer_dim: 10, level_dim: 11, sounding_dim: 657)
Dimensions without coor...ss
    platform:                  GOSAT
    sensor:                    TANSO-FTS
    spatial_resolution:         , ...]

    def _infer_concat_order_from_coords(datasets):
    
        concat_dims = []
        tile_ids = [() for ds in datasets]
    
        # All datasets have same variables because they&apos;ve been grouped as such
        ds0 = datasets[0]
        for dim in ds0.dims:
    
            # Check if dim is a coordinate dimension
            if dim in ds0:
    
                # Need to read coordinate values to do ordering
                indexes = [ds.indexes.get(dim) for ds in datasets]
                if any(index is None for index in indexes):
                    raise ValueError(
                        &quot;Every dimension needs a coordinate for &quot;
                        &quot;inferring concatenation order&quot;
                    )
    
                # If dimension coordinate values are same on every dataset then
                # should be leaving this dimension alone (it&apos;s just a &quot;bystander&quot;)
                if not all(index.equals(indexes[0]) for index in indexes[1:]):
    
                    # Infer order datasets should be arranged in along this dim
                    concat_dims.append(dim)
    
                    if all(index.is_monotonic_increasing for index in indexes):
                        ascending = True
                    elif all(index.is_monotonic_decreasing for index in indexes):
                        ascending = False
                    else:
                        raise ValueError(
                            &quot;Coordinate variable {} is neither &quot;
                            &quot;monotonically increasing nor &quot;
                            &quot;monotonically decreasing on all datasets&quot;.format(dim)
                        )
    
                    # Assume that any two datasets whose coord along dim starts
                    # with the same value have the same coord values throughout.
                    if any(index.size == 0 for index in indexes):
                        raise ValueError(&quot;Cannot handle size zero dimensions&quot;)
                    first_items = pd.Index([index.take([0]) for index in indexes])
    
                    # Sort datasets along dim
                    # We want rank but with identical elements given identical
                    # position indices - they should be concatenated along another
                    # dimension, not along this one
                    series = first_items.to_series()
                    rank = series.rank(method=&quot;dense&quot;, ascending=ascending)
                    order = rank.astype(int).values - 1
    
                    # Append positions along extra dimension to structure which
                    # encodes the multi-dimensional concatentation order
                    tile_ids = [
                        tile_id + (position,) for tile_id, position in zip(tile_ids, order)
                    ]
    
        if len(datasets) &gt; 1 and not concat_dims:
            raise ValueError(
&gt;               &quot;Could not find any dimension coordinates to use to &quot;
                &quot;order the datasets for concatenation&quot;
            )
E           ValueError: Could not find any dimension coordinates to use to order the datasets for concatenation

/home/alicja/miniconda3/envs/cate-env/lib/python3.7/site-packages/xarray/core/combine.py:109: ValueError</failure></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.day.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="307" line="94" name="test_open_local[remote_dataset165]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2018-10-29&apos;, &apos;2018-10-29&apos;)" time="5.137" time_coverage="(&apos;2018-02-28&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="K_490" dataset="esacci.OC.mon.L3S.K_490.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset166]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2009-12-01&apos;, &apos;2010-01-01&apos;)" time="32.339" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.day.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="307" line="94" name="test_open_local[remote_dataset167]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2018-08-20&apos;, &apos;2018-08-20&apos;)" time="33.282" time_coverage="(&apos;2018-02-28&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="IOP" dataset="esacci.OC.mon.L3S.IOP.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="256" line="94" name="test_open_local[remote_dataset168]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2017-03-01&apos;, &apos;2017-04-01&apos;)" time="266.842" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.day.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.4-0.sinusoidal" file="test_data_support.py" files="1000" line="94" name="test_open_local[remote_dataset169]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2015-08-26&apos;, &apos;2015-08-26&apos;)" time="54.925" time_coverage="(&apos;2015-06-04&apos;, &apos;2018-02-28&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.day.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="7763" line="94" name="test_open_local[remote_dataset170]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2007-12-21&apos;, &apos;2007-12-21&apos;)" time="83.050" time_coverage="(&apos;1997-09-04&apos;, &apos;2019-01-01&apos;)" time_frequency="day"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.5-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="736" line="94" name="test_open_local[remote_dataset171]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2012-07-29&apos;, &apos;2012-08-03&apos;)" time="296.326" time_coverage="(&apos;1997-09-09&apos;, &apos;2018-12-27&apos;)" time_frequency="5 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.8-days.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="439" line="94" name="test_open_local[remote_dataset172]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;2018-12-03&apos;, &apos;2018-12-11&apos;)" time="422.735" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-19&apos;)" time_frequency="8 days"></testcase><testcase cci_project="OC" classname="test_data_support" data_type="OC_PRODUCTS" dataset="esacci.OC.mon.L3S.OC_PRODUCTS.multi-sensor.multi-platform.MERGED.4-0.geographic" file="test_data_support.py" files="128" line="94" name="test_open_local[remote_dataset173]" processing_level="L3S" product_version="4-0" sensor_id="multi-sensor" test_time_coverage="(&apos;1999-05-01&apos;, &apos;1999-06-01&apos;)" time="522.901" time_coverage="(&apos;1997-09-04&apos;, &apos;2018-12-01&apos;)" time_frequency="month"></testcase><testcase cci_project="SEASURFACESALINITY" classname="test_data_support" data_type="SSS" dataset="esacci.SEASURFACESALINITY.15-days.L4.SSS.multi-sensor.multi-platform.MERGED_OI_Monthly_CENTRED_15Day_25km.01-08.r1" file="test_data_support.py" files="213" line="94" name="test_open_local[remote_dataset174]" processing_level="L4" product_version="01-08" sensor_id="multi-sensor" test_time_coverage="(&apos;2013-12-01&apos;, &apos;2014-02-01&apos;)" time="4.944" time_coverage="(&apos;2010-01-01&apos;, &apos;2018-11-01&apos;)" time_frequency="15 days"></testcase></testsuite></testsuites>